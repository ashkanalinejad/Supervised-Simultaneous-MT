
import torch


def pad_fixed_sequence(seq, padding=1, seq_len=7):
    pad_length = seq_len - len(seq)
    padded_sequence = seq if pad_length == 0 else torch.cat((torch.tensor([padding]*pad_length,
                                                                          device=seq.device), seq))
    return padded_sequence


def generate_incremental_net_input(hypos, num_src=7, src_pad=1, trg_pad=1):
    # bsz x src_len x feat_len
    # 'action_seq -1' is because the first action will not be generated by the Agent
    max_action_len = max( [len(hypo['action_seq']) for hypo in hypos] )
    final_input = torch.ones(len(hypos), max_action_len-1, num_src+1,
                             dtype=torch.long,
                             device=hypos[0]['src_tokens'].device)
    for i, hypo in enumerate(hypos):
        sub_input = torch.ones(len(hypo['action_seq'])-1, num_src+1)
        for j in range(1, len(hypo['action_seq'])):
            # num read and write at previous time step
            num_write = sum(hypo['action_seq'][:j])
            num_read = len(hypo['action_seq'][:j]) - num_write
            src_tokens = pad_fixed_sequence(hypo['src_tokens'][num_read-num_src:num_read] if num_read >= num_src
                                            else hypo['src_tokens'][:num_read], padding=src_pad, seq_len=num_src)

            # The last write is EOS. We just need to continue to read more source words.
            if num_write == sum(hypo['action_seq'][:]):
                num_write = num_write-1

            # The last read is EOS token. So it's the same as the lsat trg tokens
            if num_read == len(hypo['action_seq'][:]) - sum(hypo['action_seq'][:]):
                num_read = num_read-1

            # If the number of generated tokens less than the token we are looking for, then we just send
            # the padding and we expect the Agent generate read action for that time step.
            if num_write >= len(hypo['subsets'][num_read-1]['tokens']):
                trg_tokens = trg_pad
            else:
                trg_tokens = hypo['subsets'][num_read-1]['tokens'][num_write]
            input_elements = torch.cat((src_tokens, torch.tensor([trg_tokens], device=hypos[0]['src_tokens'].device)))
            sub_input[j-1] = input_elements
        final_input[i][:sub_input.shape[0]] = sub_input
    return final_input


def prepare_simultaneous_input(hypos, sample, src_dict, trg_dict, agt_dict):
    src_pad = src_dict.pad_index
    trg_pad = trg_dict.pad_index
    agt_pad = agt_dict.pad_index
    net_input = generate_incremental_net_input(hypos, num_src=7, src_pad=src_pad, trg_pad=trg_pad)
    sample['net_input']['src_tokens'] = net_input
    # The first read in the action sequence will not be generated by the Agent
    sample['net_input']['src_lengths'] = torch.tensor([len(hypo['action_seq'][1:]) for hypo in hypos], dtype=torch.long,
                                                      device=sample['net_input']['src_tokens'].device)

    sample['target'] = torch.empty(sample['net_input']['src_tokens'].shape[0],
                                   sample['net_input']['src_tokens'].shape[1], dtype=torch.long,
                                   device=sample['net_input']['src_tokens'].device).fill_(agt_pad)
    sample['net_input']['prev_output_tokens'] = torch.empty(sample['net_input']['src_tokens'].shape[0],
                                                            sample['net_input']['src_tokens'].shape[1],
                                                            dtype=torch.long,
                                                            device=sample['net_input']['src_tokens'].device
                                                            ).fill_(agt_pad)
    for i, hypo in enumerate(hypos):
        encoded_action = agt_dict.encode_line(" ".join(str(elem) for elem in hypo['action_seq']), append_eos=False)
        sample['target'][i][:len(hypo['action_seq'][1:])] = encoded_action[1:]
        sample['net_input']['prev_output_tokens'][i][:len(hypo['action_seq'][:-1])] = encoded_action[:-1]
    return sample
