# @package _group_
nmt_task._name:  'Supervised_simultaneous_translation'
all_gather_list_size: 16384
azureml_logging: False
batch_size: None
batch_size_valid: None
beam: 2
best_checkpoint_metric: 'loss'
bf16: False
bpe: None
broadcast_buffers: False
bucket_cap_mb: 25
checkpoint_shard_count: 1
checkpoint_suffix: ''
constraints: None
cpu: False
criterion: 'cross_entropy'
curriculum: 0
data: '/Users/alinejad/Desktop/SFU/Research/Speech-to-text-transation/Supervised-fairseq/data-bin/wmt14_en_de/bin-de_en-encoded'
data_buffer_size: 10
dataset_impl: None
ddp_backend: 'c10d'
decoding_format: None
disable_validation: False
distributed_backend: 'nccl'
distributed_init_method: None
distributed_no_spawn: False
distributed_port: -1
distributed_rank: 0
distributed_world_size: 1
distributed_wrapper: 'DDP'
diverse_beam_groups: -1
diverse_beam_strength: 0.5
diversity_rate: -1.0
empty_cache_freq: 0
eos: 2
eval_bleu: False
eval_bleu_args: None
eval_bleu_detok: 'space'
eval_bleu_detok_args: None
eval_bleu_print_samples: False
eval_bleu_remove_bpe: None
eval_tokenized_bleu: False
fast_stat_sync: False
find_unused_parameters: False
finetune_from_model: None
fix_batches_to_gpus: False
fixed_validation_seed: None
force_anneal: None
fp16: False
fp16_init_scale: 128
fp16_no_flatten_grads: False
fp16_scale_tolerance: 0.0
fp16_scale_window: None
gen_subset: 'valid'
iter_decode_eos_penalty: 0.0
iter_decode_force_max_iter: False
iter_decode_max_iter: 10
iter_decode_with_beam: 1
iter_decode_with_external_reranker: False
keep_best_checkpoints: -1
keep_interval_updates: -1
keep_last_epochs: -1
left_pad_source: 'False'
left_pad_target: 'False'
lenpen: 1
lm_path: None
lm_weight: 0.0
load_alignments: False
load_checkpoint_on_all_dp_ranks: False
localsgd_frequency: 3
log_format: None
log_interval: 100
lr_scheduler: 'fixed'
lr_shrink: 0.1
match_source_len: False
max_len_a: 0
max_len_b: 200
max_source_positions: 1024
max_target_positions: 1024
max_tokens: 8000
max_tokens_valid: 8000
maximize_best_checkpoint_metric: False
memory_efficient_bf16: False
memory_efficient_fp16: False
min_len: 1
min_loss_scale: 0.0001
model_overrides: '{}'
model_parallel_size: 1
nbest: 1
no_beamable_mm: False
no_early_stop: False
no_epoch_checkpoints: False
no_last_checkpoints: False
no_progress_bar: False
no_repeat_ngram_size: 0
no_save: False
no_save_optimizer_state: False
no_seed_provided: False
nprocs_per_node: 1
num_batch_buckets: 0
num_shards: 1
num_workers: 1
optimizer: None
optimizer_overrides: '{}'
pad: 1
path: '/Users/alinejad/Desktop/SFU/Research/Speech-to-text-transation/Supervised-fairseq/nmt_trans_wmt14_deen_med/checkpoint_best.pt'
patience: -1
pipeline_balance: None
pipeline_checkpoint: 'never'
pipeline_chunks: 0
pipeline_decoder_balance: None
pipeline_decoder_devices: None
pipeline_devices: None
pipeline_encoder_balance: None
pipeline_encoder_devices: None
pipeline_model_parallel: False
post_process: None
prefix_size: 0
print_alignment: None
print_step: False
profile: False
quantization_config_path: None
quiet: False
replace_unk: None
required_batch_size_multiple: 8
required_seq_len_multiple: 1
reset_dataloader: False
reset_logging: True
reset_lr_scheduler: False
reset_meters: False
reset_optimizer: False
restore_file: 'checkpoint_last.pt'
results_path: None
retain_dropout: False
retain_dropout_modules: None
retain_iter_history: False
sacrebleu: False
sampling: False
sampling_topk: -1
sampling_topp: -1.0
save_dir: 'checkpoints'
save_interval: 1
save_interval_updates: 0
score_reference: False
scoring: 'bleu'
seed: 1
shard_id: 0
skip_invalid_size_inputs_valid_test: True
slowmo_algorithm: 'LocalSGD'
slowmo_momentum: None
source_lang: 'de'
target_lang: 'en'
task: 'Supervised_simultaneous_translation'
temperature: 1.0
tensorboard_logdir: None
threshold_loss_scale: None
tokenizer: None
tpu: False
train_subset: 'train'
truncate_source: False
unk: 3
unkpen: 0
unnormalized: False
upsample_primary: 1
user_dir: '../examples/Supervised_simul_MT'
valid_subset: 'valid'
validate_after_updates: 0
validate_interval: 1
validate_interval_updates: 0
wandb_project: None
warmup_updates: 0
zero_sharding: 'none'
    